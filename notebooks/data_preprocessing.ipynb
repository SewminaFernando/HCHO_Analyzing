{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql as pyspark_sql\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession creation\n",
    "spark = pyspark_sql.SparkSession.builder.appName(\"Data_Preprocessing\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing Datasets according to the Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. col_mat_nuw_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data with setting column names\n",
    "df1 = spark.read.csv(\"..\\data\\col_mat_nuw_output.csv\", header=False, inferSchema=True)\n",
    "df1 = df1.withColumnRenamed(\"_c0\", \"hcho_reading\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"location\") \\\n",
    "    .withColumnRenamed(\"_c2\", \"current_date\") \\\n",
    "    .withColumnRenamed(\"_c3\", \"next_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hcho_reading: double (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- current_date: date (nullable = true)\n",
      " |-- next_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+------------+----------+\n",
      "|hcho_reading         |location      |current_date|next_date |\n",
      "+---------------------+--------------+------------+----------+\n",
      "|1.9698343957810148E-4|Colombo Proper|2019-01-01  |2019-01-02|\n",
      "|2.6255221719685945E-4|Colombo Proper|2019-01-02  |2019-01-03|\n",
      "|9.852118897938794E-5 |Colombo Proper|2019-01-03  |2019-01-04|\n",
      "|2.099320518114242E-4 |Colombo Proper|2019-01-04  |2019-01-05|\n",
      "|1.7853372988929305E-4|Colombo Proper|2019-01-05  |2019-01-06|\n",
      "|1.0822967002356709E-4|Colombo Proper|2019-01-06  |2019-01-07|\n",
      "|3.9268292804773094E-4|Colombo Proper|2019-01-07  |2019-01-08|\n",
      "|9.153156350685351E-5 |Colombo Proper|2019-01-08  |2019-01-09|\n",
      "|1.2059789928530154E-4|Colombo Proper|2019-01-09  |2019-01-10|\n",
      "|1.2977235629832586E-4|Colombo Proper|2019-01-10  |2019-01-11|\n",
      "|2.2391881668012785E-4|Colombo Proper|2019-01-11  |2019-01-12|\n",
      "|1.5694180941787597E-4|Colombo Proper|2019-01-12  |2019-01-13|\n",
      "|NULL                 |Colombo Proper|2019-01-13  |2019-01-14|\n",
      "|1.3362919068626037E-4|Colombo Proper|2019-01-14  |2019-01-15|\n",
      "|6.374417842690063E-5 |Colombo Proper|2019-01-15  |2019-01-16|\n",
      "|1.1810622508150202E-4|Colombo Proper|2019-01-16  |2019-01-17|\n",
      "|2.4725552224230375E-4|Colombo Proper|2019-01-17  |2019-01-18|\n",
      "|3.667525352047757E-5 |Colombo Proper|2019-01-18  |2019-01-19|\n",
      "|4.057500868150313E-4 |Colombo Proper|2019-01-19  |2019-01-20|\n",
      "|1.6878562164797227E-4|Colombo Proper|2019-01-20  |2019-01-21|\n",
      "+---------------------+--------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows:  5478\n"
     ]
    }
   ],
   "source": [
    "# No of rows\n",
    "print(\"No of rows: \", df1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Negative values to null values\n",
    "df1 = df1.withColumn(\"hcho_reading\", F.when(F.col(\"hcho_reading\") < 0, None).otherwise(F.col(\"hcho_reading\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------+---------+\n",
      "|hcho_reading|location|current_date|next_date|\n",
      "+------------+--------+------------+---------+\n",
      "|        2682|       0|           0|        0|\n",
      "+------------+--------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of null values in each column\n",
    "df1.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df1.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------+---------------------+------------------+\n",
      "|(hcho_reading / 5478)|(location / 5478)|(current_date / 5478)|(next_date / 5478)|\n",
      "+---------------------+-----------------+---------------------+------------------+\n",
      "|   0.4895947426067908|              0.0|                  0.0|               0.0|\n",
      "+---------------------+-----------------+---------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As a percentage get the number of null values in each column\n",
    "df1.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df1.columns]) \\\n",
    "    .select([F.col(c) / df1.count() for c in df1.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|           location|count|\n",
      "+-------------------+-----+\n",
      "|   Deniyaya, Matara| 1826|\n",
      "|     Colombo Proper| 1826|\n",
      "|Nuwara Eliya Proper| 1826|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by location and get the count of each location\n",
    "df1.groupBy(\"location\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devide the dataset according to the location and save as dataframes\n",
    "df_colombo = df1.filter(df1.location == \"Colombo Proper\").toPandas()\n",
    "df_matara = df1.filter(df1.location == \"Deniyaya, Matara\").toPandas()\n",
    "df_nuwara_eliya = df1.filter(df1.location == \"Nuwara Eliya Proper\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colombo Proper: 1826\n",
      "Deniyaya, Matara: 1826\n",
      "Nuwara Eliya Proper: 1826\n"
     ]
    }
   ],
   "source": [
    "# Get the length of each dataframe\n",
    "print(\"Colombo Proper:\", len(df_colombo))\n",
    "print(\"Deniyaya, Matara:\", len(df_matara))\n",
    "print(\"Nuwara Eliya Proper:\", len(df_nuwara_eliya))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframes to csv files with dropping the location column\n",
    "df_colombo.drop(columns=[\"location\"]).to_csv(r\"..\\data\\location\\colombo_proper.csv\", index=False)\n",
    "df_matara.drop(columns=[\"location\"]).to_csv(r\"..\\data\\location\\deniyaya_matara.csv\", index=False)\n",
    "df_nuwara_eliya.drop(columns=[\"location\"]).to_csv(r\"..\\data\\location\\nuwaraeliya_proper.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. kan_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv(\"..\\data\\kan_output.csv\", header=False, inferSchema=True)\n",
    "df2 = df2.withColumnRenamed(\"_c0\", \"hcho_reading\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"location\") \\\n",
    "    .withColumnRenamed(\"_c2\", \"current_date\") \\\n",
    "    .withColumnRenamed(\"_c3\", \"next_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hcho_reading: double (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- current_date: date (nullable = true)\n",
      " |-- next_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------+------------+----------+\n",
      "|hcho_reading          |location    |current_date|next_date |\n",
      "+----------------------+------------+------------+----------+\n",
      "|1.7607134598773356E-4 |Kandy Proper|2019-01-01  |2019-01-02|\n",
      "|9.220391253917748E-5  |Kandy Proper|2019-01-02  |2019-01-03|\n",
      "|NULL                  |Kandy Proper|2019-01-03  |2019-01-04|\n",
      "|1.9086819838538396E-4 |Kandy Proper|2019-01-04  |2019-01-05|\n",
      "|1.2195178402067448E-4 |Kandy Proper|2019-01-05  |2019-01-06|\n",
      "|-6.514086129388805E-5 |Kandy Proper|2019-01-06  |2019-01-07|\n",
      "|1.6323820639265E-4    |Kandy Proper|2019-01-07  |2019-01-08|\n",
      "|-6.735205533914268E-5 |Kandy Proper|2019-01-08  |2019-01-09|\n",
      "|1.2796936582431357E-4 |Kandy Proper|2019-01-09  |2019-01-10|\n",
      "|4.546048424126012E-5  |Kandy Proper|2019-01-10  |2019-01-11|\n",
      "|3.600074175192105E-5  |Kandy Proper|2019-01-11  |2019-01-12|\n",
      "|1.286629698010177E-4  |Kandy Proper|2019-01-12  |2019-01-13|\n",
      "|NULL                  |Kandy Proper|2019-01-13  |2019-01-14|\n",
      "|NULL                  |Kandy Proper|2019-01-14  |2019-01-15|\n",
      "|9.63639634671553E-5   |Kandy Proper|2019-01-15  |2019-01-16|\n",
      "|NULL                  |Kandy Proper|2019-01-16  |2019-01-17|\n",
      "|1.2009712784847367E-4 |Kandy Proper|2019-01-17  |2019-01-18|\n",
      "|-1.1972465671369591E-4|Kandy Proper|2019-01-18  |2019-01-19|\n",
      "|2.8252637615294707E-4 |Kandy Proper|2019-01-19  |2019-01-20|\n",
      "|7.150631692646835E-5  |Kandy Proper|2019-01-20  |2019-01-21|\n",
      "+----------------------+------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows:  1826\n"
     ]
    }
   ],
   "source": [
    "# No of rows\n",
    "print(\"No of rows: \", df2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Negative values to null values\n",
    "df2 = df2.withColumn(\"hcho_reading\", F.when(F.col(\"hcho_reading\") < 0, None).otherwise(F.col(\"hcho_reading\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------+---------+\n",
      "|hcho_reading|location|current_date|next_date|\n",
      "+------------+--------+------------+---------+\n",
      "|         901|       0|           0|        0|\n",
      "+------------+--------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of null values in each column\n",
    "df2.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df2.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------+---------------------+------------------+\n",
      "|(hcho_reading / 1826)|(location / 1826)|(current_date / 1826)|(next_date / 1826)|\n",
      "+---------------------+-----------------+---------------------+------------------+\n",
      "|  0.49342825848849947|              0.0|                  0.0|               0.0|\n",
      "+---------------------+-----------------+---------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As a percentage get the number of null values in each column\n",
    "df2.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df2.columns]) \\\n",
    "    .select([F.col(c) / df2.count() for c in df2.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|    location|\n",
      "+------------+\n",
      "|Kandy Proper|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the distinct values in the location column\n",
    "df2.select(\"location\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV file\n",
    "df2.toPandas().drop(columns=[\"location\"]).to_csv(r\"..\\data\\location\\kandy_proper.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. mon_kur_jaf_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.read.csv(\"..\\data\\mon_kur_jaf_output.csv\", header=False, inferSchema=True)\n",
    "df3 = df3.withColumnRenamed(\"_c0\", \"hcho_reading\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"location\") \\\n",
    "    .withColumnRenamed(\"_c2\", \"current_date\") \\\n",
    "    .withColumnRenamed(\"_c3\", \"next_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hcho_reading: double (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- current_date: date (nullable = true)\n",
      " |-- next_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+------------+----------+\n",
      "|hcho_reading          |location          |current_date|next_date |\n",
      "+----------------------+------------------+------------+----------+\n",
      "|NULL                  |Bibile, Monaragala|2019-01-01  |2019-01-02|\n",
      "|1.919914652467399E-5  |Bibile, Monaragala|2019-01-02  |2019-01-03|\n",
      "|2.8114479359302837E-5 |Bibile, Monaragala|2019-01-03  |2019-01-04|\n",
      "|3.747998184385943E-5  |Bibile, Monaragala|2019-01-04  |2019-01-05|\n",
      "|-1.7982608793453114E-5|Bibile, Monaragala|2019-01-05  |2019-01-06|\n",
      "|1.4578368961799026E-4 |Bibile, Monaragala|2019-01-06  |2019-01-07|\n",
      "|2.8285908025465342E-5 |Bibile, Monaragala|2019-01-07  |2019-01-08|\n",
      "|NULL                  |Bibile, Monaragala|2019-01-08  |2019-01-09|\n",
      "|1.4208501670509577E-4 |Bibile, Monaragala|2019-01-09  |2019-01-10|\n",
      "|NULL                  |Bibile, Monaragala|2019-01-10  |2019-01-11|\n",
      "|2.014587947072581E-5  |Bibile, Monaragala|2019-01-11  |2019-01-12|\n",
      "|1.5827876632101837E-4 |Bibile, Monaragala|2019-01-12  |2019-01-13|\n",
      "|NULL                  |Bibile, Monaragala|2019-01-13  |2019-01-14|\n",
      "|NULL                  |Bibile, Monaragala|2019-01-14  |2019-01-15|\n",
      "|9.952823849642218E-5  |Bibile, Monaragala|2019-01-15  |2019-01-16|\n",
      "|7.484570960514249E-5  |Bibile, Monaragala|2019-01-16  |2019-01-17|\n",
      "|7.736112226328797E-5  |Bibile, Monaragala|2019-01-17  |2019-01-18|\n",
      "|NULL                  |Bibile, Monaragala|2019-01-18  |2019-01-19|\n",
      "|3.9881410104076654E-5 |Bibile, Monaragala|2019-01-19  |2019-01-20|\n",
      "|1.8916202038533428E-4 |Bibile, Monaragala|2019-01-20  |2019-01-21|\n",
      "+----------------------+------------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of rows:  5478\n"
     ]
    }
   ],
   "source": [
    "# No of rows\n",
    "print(\"No of rows: \", df3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Negative values to null values\n",
    "df3 = df3.withColumn(\"hcho_reading\", F.when(F.col(\"hcho_reading\") < 0, None).otherwise(F.col(\"hcho_reading\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------------+---------+\n",
      "|hcho_reading|location|current_date|next_date|\n",
      "+------------+--------+------------+---------+\n",
      "|        1844|       0|           0|        0|\n",
      "+------------+--------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of null values in each column\n",
    "df3.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df3.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------+---------------------+------------------+\n",
      "|(hcho_reading / 5478)|(location / 5478)|(current_date / 5478)|(next_date / 5478)|\n",
      "+---------------------+-----------------+---------------------+------------------+\n",
      "|  0.33661920408908363|              0.0|                  0.0|               0.0|\n",
      "+---------------------+-----------------+---------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As a percentage get the number of null values in each column\n",
    "df3.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in df3.columns]) \\\n",
    "    .select([F.col(c) / df3.count() for c in df3.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|          location|count|\n",
      "+------------------+-----+\n",
      "| Kurunegala Proper| 1826|\n",
      "|Bibile, Monaragala| 1826|\n",
      "|     Jaffna Proper| 1826|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by location and get the count of each location\n",
    "df3.groupBy(\"location\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devide the dataset according to the location and save as dataframes\n",
    "df_kurunegala = df3.filter(df3.location == \"Kurunegala Proper\").toPandas()\n",
    "df_monaragala = df3.filter(df3.location == \"Bibile, Monaragala\").toPandas()\n",
    "df_jaffna = df3.filter(df3.location == \"Jaffna Proper\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kurunegala Proper: 1826\n",
      "Bibile, Monaragala: 1826\n",
      "Jaffna Proper: 1826\n"
     ]
    }
   ],
   "source": [
    "# Get the length of each dataframe\n",
    "print(\"Kurunegala Proper:\", len(df_kurunegala))\n",
    "print(\"Bibile, Monaragala:\", len(df_monaragala))\n",
    "print(\"Jaffna Proper:\", len(df_jaffna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframes to csv files with dropping the location column\n",
    "df_kurunegala.drop(columns=[\"location\"]).to_csv(r\"..\\data\\location\\kurunegala_proper.csv\", index=False)\n",
    "df_monaragala.drop(columns=[\"location\"]).to_csv(r\"..\\data\\location\\bibile_monaragala.csv\", index=False)\n",
    "df_jaffna.drop(columns=[\"location\"]).to_csv(r\"..\\data\\location\\jaffna_proper.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all dataframes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df1, df2 and df3\n",
    "df = df1.union(df2).union(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------+------------+----------+\n",
      "|hcho_reading         |location      |current_date|next_date |\n",
      "+---------------------+--------------+------------+----------+\n",
      "|1.9698343957810148E-4|Colombo Proper|2019-01-01  |2019-01-02|\n",
      "|2.6255221719685945E-4|Colombo Proper|2019-01-02  |2019-01-03|\n",
      "|9.852118897938794E-5 |Colombo Proper|2019-01-03  |2019-01-04|\n",
      "|2.099320518114242E-4 |Colombo Proper|2019-01-04  |2019-01-05|\n",
      "|1.7853372988929305E-4|Colombo Proper|2019-01-05  |2019-01-06|\n",
      "|1.0822967002356709E-4|Colombo Proper|2019-01-06  |2019-01-07|\n",
      "|3.9268292804773094E-4|Colombo Proper|2019-01-07  |2019-01-08|\n",
      "|9.153156350685351E-5 |Colombo Proper|2019-01-08  |2019-01-09|\n",
      "|1.2059789928530154E-4|Colombo Proper|2019-01-09  |2019-01-10|\n",
      "|1.2977235629832586E-4|Colombo Proper|2019-01-10  |2019-01-11|\n",
      "|2.2391881668012785E-4|Colombo Proper|2019-01-11  |2019-01-12|\n",
      "|1.5694180941787597E-4|Colombo Proper|2019-01-12  |2019-01-13|\n",
      "|NULL                 |Colombo Proper|2019-01-13  |2019-01-14|\n",
      "|1.3362919068626037E-4|Colombo Proper|2019-01-14  |2019-01-15|\n",
      "|6.374417842690063E-5 |Colombo Proper|2019-01-15  |2019-01-16|\n",
      "|1.1810622508150202E-4|Colombo Proper|2019-01-16  |2019-01-17|\n",
      "|2.4725552224230375E-4|Colombo Proper|2019-01-17  |2019-01-18|\n",
      "|3.667525352047757E-5 |Colombo Proper|2019-01-18  |2019-01-19|\n",
      "|4.057500868150313E-4 |Colombo Proper|2019-01-19  |2019-01-20|\n",
      "|1.6878562164797227E-4|Colombo Proper|2019-01-20  |2019-01-21|\n",
      "+---------------------+--------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire Dataset Count: 12782\n"
     ]
    }
   ],
   "source": [
    "print(\"Entire Dataset Count:\",df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|           location|count|\n",
      "+-------------------+-----+\n",
      "|   Deniyaya, Matara| 1826|\n",
      "|     Colombo Proper| 1826|\n",
      "|Nuwara Eliya Proper| 1826|\n",
      "|       Kandy Proper| 1826|\n",
      "|  Kurunegala Proper| 1826|\n",
      "| Bibile, Monaragala| 1826|\n",
      "|      Jaffna Proper| 1826|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by location and get the count of each location\n",
    "df.groupBy(\"location\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged dataframe to a csv file\n",
    "df.toPandas().to_csv(r\"..\\data\\all_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
